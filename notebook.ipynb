{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/opyate/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model, Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "# our stuff\n",
    "from synth import *\n",
    "from utils import *\n",
    "from const import Tx, Ty, n_a, n_s, input_vocab, output_vocab, output_vocab_inv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/device:CPU:0\n",
      "/device:GPU:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "[print(x.name) for x in device_lib.list_local_devices()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"twenty five minutes and fifty five seconds past three o'clock in the morning\",\n",
       " '03:25:55',\n",
       " datetime.time(3, 25, 55))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns input,output,dt\n",
    "# Run once to see an example.\n",
    "load_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 18 hours, 8 mins and 34 seconds\n",
      "Target: 18:08:34\n",
      "Input shape: (1, 128, 102)\n",
      "Target shape: (1, 8, 11)\n",
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# One-hot representations of load_time()'s input and output\n",
    "# will be fed into the model.\n",
    "\n",
    "i, o, _ = load_time()\n",
    "\n",
    "print('Input:', i)\n",
    "print('Target:', o)\n",
    "\n",
    "ioh, ooh = one_hot([i], [o], Tx, Ty, input_vocab, output_vocab)\n",
    "\n",
    "print('Input shape:', ioh.shape)\n",
    "print('Target shape:', ooh.shape)\n",
    "print(*ooh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ioh, \"input one-hot\", has shape (128,102), i.e. it can accept an input string of up to (or padded to) 128 characters of a possible vocabulary of 102 characters.\n",
    "- ooh, \"output one-hot\", has shape (8,11), i.e. the model will predict 8 characters of the form 'hh:mm:ss' of a possible vocabulary of all digits and colon ':'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defined shared layers as global variables\n",
    "repeator = RepeatVector(Tx)\n",
    "concatenator = Concatenate(axis=-1)\n",
    "densor = Dense(1, activation = \"relu\")\n",
    "# softmax(axis = 1) from utils.py\n",
    "activator = Activation(softmax, name='attention_weights')\n",
    "dotor = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_step_attention(a, s_prev):\n",
    "    \"\"\"\n",
    "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
    "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
    "    \n",
    "    Arguments:\n",
    "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
    "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
    "    \n",
    "    Returns:\n",
    "    context -- context vector, input of the next (post-attetion) LSTM cell\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states \"a\"\n",
    "    s_prev = repeator(s_prev)\n",
    "    # Use concatenator to concatenate a and s_prev on the last axis\n",
    "    concat = concatenator([a, s_prev])\n",
    "    # Use densor to propagate concat through a small fully-connected neural network to compute the \"energies\" variable e.\n",
    "    e = densor(concat)\n",
    "    # Use activator and e to compute the attention weights \"alphas\"\n",
    "    alphas = activator(e)\n",
    "    # Use dotor together with \"alphas\" and \"a\" to compute the context vector to be given to the next (post-attention) LSTM-cell\n",
    "    context = dotor([alphas, a])\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "post_activation_LSTM_cell = LSTM(n_s, return_state = True)\n",
    "output_layer = Dense(len(output_vocab), activation=softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Tx -- length of the input sequence\n",
    "    Ty -- length of the output sequence\n",
    "    n_a -- hidden state size of the Bi-LSTM\n",
    "    n_s -- hidden state size of the post-attention LSTM\n",
    "    human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
    "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the inputs of your model with a shape (Tx,)\n",
    "    # Define s0 and c0, initial hidden state for the decoder LSTM of shape (n_s,)\n",
    "    X = Input(shape=(Tx, human_vocab_size))\n",
    "    s0 = Input(shape=(n_s,), name='s0')\n",
    "    c0 = Input(shape=(n_s,), name='c0')\n",
    "    s = s0\n",
    "    c = c0\n",
    "    \n",
    "    # Initialize empty list of outputs\n",
    "    outputs = []\n",
    "    \n",
    "    # Step 1: Define pre-attention Bi-LSTM. Remember to use return_sequences=True.\n",
    "    a = Bidirectional(LSTM(n_a, return_sequences=True))(X)\n",
    "    \n",
    "    # Step 2: Iterate for Ty steps\n",
    "    for t in range(Ty):\n",
    "    \n",
    "        # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t\n",
    "        context = one_step_attention(a, s)\n",
    "        \n",
    "        # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector.\n",
    "        s, _, c = post_activation_LSTM_cell(context, initial_state=[s, c])\n",
    "        \n",
    "        # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM\n",
    "        out = output_layer(s)\n",
    "        \n",
    "        # Step 2.D: Append \"out\" to the \"outputs\" list\n",
    "        outputs.append(out)\n",
    "    \n",
    "    # Step 3: Create model instance taking three inputs and returning the list of outputs.\n",
    "    model = Model(inputs=[X, s0, c0], outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = get_model(Tx, Ty, n_a, n_s, len(input_vocab), len(output_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 128, 102)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "s0 (InputLayer)                 (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 128, 128)     85504       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 128, 128)     0           s0[0][0]                         \n",
      "                                                                 lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[3][0]                     \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[5][0]                     \n",
      "                                                                 lstm_1[6][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128, 256)     0           bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[0][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[1][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[2][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[3][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[4][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[5][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[6][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[7][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128, 1)       257         concatenate_1[0][0]              \n",
      "                                                                 concatenate_1[1][0]              \n",
      "                                                                 concatenate_1[2][0]              \n",
      "                                                                 concatenate_1[3][0]              \n",
      "                                                                 concatenate_1[4][0]              \n",
      "                                                                 concatenate_1[5][0]              \n",
      "                                                                 concatenate_1[6][0]              \n",
      "                                                                 concatenate_1[7][0]              \n",
      "__________________________________________________________________________________________________\n",
      "attention_weights (Activation)  (None, 128, 1)       0           dense_1[0][0]                    \n",
      "                                                                 dense_1[1][0]                    \n",
      "                                                                 dense_1[2][0]                    \n",
      "                                                                 dense_1[3][0]                    \n",
      "                                                                 dense_1[4][0]                    \n",
      "                                                                 dense_1[5][0]                    \n",
      "                                                                 dense_1[6][0]                    \n",
      "                                                                 dense_1[7][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1, 128)       0           attention_weights[0][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[1][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[2][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[3][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[4][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[5][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[6][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[7][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "c0 (InputLayer)                 (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 128), (None, 131584      dot_1[0][0]                      \n",
      "                                                                 s0[0][0]                         \n",
      "                                                                 c0[0][0]                         \n",
      "                                                                 dot_1[1][0]                      \n",
      "                                                                 lstm_1[0][0]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "                                                                 dot_1[2][0]                      \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[1][2]                     \n",
      "                                                                 dot_1[3][0]                      \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[2][2]                     \n",
      "                                                                 dot_1[4][0]                      \n",
      "                                                                 lstm_1[3][0]                     \n",
      "                                                                 lstm_1[3][2]                     \n",
      "                                                                 dot_1[5][0]                      \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[4][2]                     \n",
      "                                                                 dot_1[6][0]                      \n",
      "                                                                 lstm_1[5][0]                     \n",
      "                                                                 lstm_1[5][2]                     \n",
      "                                                                 dot_1[7][0]                      \n",
      "                                                                 lstm_1[6][0]                     \n",
      "                                                                 lstm_1[6][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 11)           1419        lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[3][0]                     \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[5][0]                     \n",
      "                                                                 lstm_1[6][0]                     \n",
      "                                                                 lstm_1[7][0]                     \n",
      "==================================================================================================\n",
      "Total params: 218,764\n",
      "Trainable params: 218,764\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.005, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1000/1000 [==============================] - 251s 251ms/step - loss: 6.7304 - dense_2_loss_1: 0.6983 - dense_2_loss_2: 1.7767 - dense_2_loss_3: 0.0563 - dense_2_loss_4: 0.9313 - dense_2_loss_5: 1.1720 - dense_2_loss_6: 0.0831 - dense_2_loss_7: 0.9671 - dense_2_loss_8: 1.0457 - dense_2_acc_1: 0.6624 - dense_2_acc_2: 0.3198 - dense_2_acc_3: 0.9990 - dense_2_acc_4: 0.6168 - dense_2_acc_5: 0.5652 - dense_2_acc_6: 0.9990 - dense_2_acc_7: 0.5942 - dense_2_acc_8: 0.6026\n",
      "Epoch 2/1000\n",
      "1000/1000 [==============================] - 248s 248ms/step - loss: 1.1448 - dense_2_loss_1: 0.2080 - dense_2_loss_2: 0.5064 - dense_2_loss_3: 9.8783e-04 - dense_2_loss_4: 0.1322 - dense_2_loss_5: 0.1285 - dense_2_loss_6: 9.7126e-04 - dense_2_loss_7: 0.1063 - dense_2_loss_8: 0.0614 - dense_2_acc_1: 0.9203 - dense_2_acc_2: 0.8650 - dense_2_acc_3: 1.0000 - dense_2_acc_4: 0.9595 - dense_2_acc_5: 0.9685 - dense_2_acc_6: 1.0000 - dense_2_acc_7: 0.9758 - dense_2_acc_8: 0.9891\n",
      "Epoch 3/1000\n",
      "1000/1000 [==============================] - 247s 247ms/step - loss: 0.3507 - dense_2_loss_1: 0.0609 - dense_2_loss_2: 0.1429 - dense_2_loss_3: 5.3887e-04 - dense_2_loss_4: 0.0455 - dense_2_loss_5: 0.0500 - dense_2_loss_6: 5.0093e-04 - dense_2_loss_7: 0.0310 - dense_2_loss_8: 0.0192 - dense_2_acc_1: 0.9928 - dense_2_acc_2: 0.9862 - dense_2_acc_3: 1.0000 - dense_2_acc_4: 0.9916 - dense_2_acc_5: 0.9871 - dense_2_acc_6: 1.0000 - dense_2_acc_7: 0.9955 - dense_2_acc_8: 0.9976\n",
      "Epoch 4/1000\n",
      "1000/1000 [==============================] - 247s 247ms/step - loss: 0.1633 - dense_2_loss_1: 0.0250 - dense_2_loss_2: 0.0637 - dense_2_loss_3: 2.9768e-04 - dense_2_loss_4: 0.0225 - dense_2_loss_5: 0.0287 - dense_2_loss_6: 3.0060e-04 - dense_2_loss_7: 0.0140 - dense_2_loss_8: 0.0088 - dense_2_acc_1: 0.9994 - dense_2_acc_2: 0.9949 - dense_2_acc_3: 1.0000 - dense_2_acc_4: 0.9978 - dense_2_acc_5: 0.9907 - dense_2_acc_6: 1.0000 - dense_2_acc_7: 0.9987 - dense_2_acc_8: 0.9993\n",
      "Epoch 5/1000\n",
      "1000/1000 [==============================] - 247s 247ms/step - loss: 0.0963 - dense_2_loss_1: 0.0133 - dense_2_loss_2: 0.0354 - dense_2_loss_3: 1.8707e-04 - dense_2_loss_4: 0.0135 - dense_2_loss_5: 0.0210 - dense_2_loss_6: 1.9836e-04 - dense_2_loss_7: 0.0078 - dense_2_loss_8: 0.0049 - dense_2_acc_1: 0.9998 - dense_2_acc_2: 0.9972 - dense_2_acc_3: 1.0000 - dense_2_acc_4: 0.9988 - dense_2_acc_5: 0.9915 - dense_2_acc_6: 1.0000 - dense_2_acc_7: 0.9995 - dense_2_acc_8: 0.9998\n",
      "Epoch 6/1000\n",
      "1000/1000 [==============================] - 247s 247ms/step - loss: 0.0663 - dense_2_loss_1: 0.0082 - dense_2_loss_2: 0.0226 - dense_2_loss_3: 1.2140e-04 - dense_2_loss_4: 0.0087 - dense_2_loss_5: 0.0176 - dense_2_loss_6: 1.3449e-04 - dense_2_loss_7: 0.0054 - dense_2_loss_8: 0.0034 - dense_2_acc_1: 0.9998 - dense_2_acc_2: 0.9982 - dense_2_acc_3: 1.0000 - dense_2_acc_4: 0.9993 - dense_2_acc_5: 0.9917 - dense_2_acc_6: 1.0000 - dense_2_acc_7: 0.9996 - dense_2_acc_8: 0.9998\n",
      "Epoch 7/1000\n",
      "1000/1000 [==============================] - 248s 248ms/step - loss: 0.0476 - dense_2_loss_1: 0.0055 - dense_2_loss_2: 0.0149 - dense_2_loss_3: 8.3943e-05 - dense_2_loss_4: 0.0059 - dense_2_loss_5: 0.0157 - dense_2_loss_6: 9.7496e-05 - dense_2_loss_7: 0.0033 - dense_2_loss_8: 0.0021 - dense_2_acc_1: 0.9999 - dense_2_acc_2: 0.9990 - dense_2_acc_3: 1.0000 - dense_2_acc_4: 0.9996 - dense_2_acc_5: 0.9917 - dense_2_acc_6: 1.0000 - dense_2_acc_7: 0.9999 - dense_2_acc_8: 0.9999\n",
      "Epoch 8/1000\n",
      "1000/1000 [==============================] - 248s 248ms/step - loss: 0.0379 - dense_2_loss_1: 0.0039 - dense_2_loss_2: 0.0106 - dense_2_loss_3: 6.0469e-05 - dense_2_loss_4: 0.0042 - dense_2_loss_5: 0.0147 - dense_2_loss_6: 7.0729e-05 - dense_2_loss_7: 0.0026 - dense_2_loss_8: 0.0017 - dense_2_acc_1: 0.9999 - dense_2_acc_2: 0.9993 - dense_2_acc_3: 1.0000 - dense_2_acc_4: 0.9998 - dense_2_acc_5: 0.9916 - dense_2_acc_6: 1.0000 - dense_2_acc_7: 0.9998 - dense_2_acc_8: 0.9999\n",
      "Epoch 9/1000\n",
      "1000/1000 [==============================] - 248s 248ms/step - loss: 0.0342 - dense_2_loss_1: 0.0035 - dense_2_loss_2: 0.0105 - dense_2_loss_3: 4.4807e-05 - dense_2_loss_4: 0.0032 - dense_2_loss_5: 0.0138 - dense_2_loss_6: 5.0383e-05 - dense_2_loss_7: 0.0019 - dense_2_loss_8: 0.0012 - dense_2_acc_1: 0.9998 - dense_2_acc_2: 0.9987 - dense_2_acc_3: 1.0000 - dense_2_acc_4: 0.9998 - dense_2_acc_5: 0.9918 - dense_2_acc_6: 1.0000 - dense_2_acc_7: 0.9999 - dense_2_acc_8: 1.0000\n",
      "Epoch 10/1000\n",
      "1000/1000 [==============================] - 248s 248ms/step - loss: 0.0263 - dense_2_loss_1: 0.0023 - dense_2_loss_2: 0.0063 - dense_2_loss_3: 3.1890e-05 - dense_2_loss_4: 0.0023 - dense_2_loss_5: 0.0132 - dense_2_loss_6: 3.7682e-05 - dense_2_loss_7: 0.0013 - dense_2_loss_8: 8.5204e-04 - dense_2_acc_1: 0.9999 - dense_2_acc_2: 0.9997 - dense_2_acc_3: 1.0000 - dense_2_acc_4: 0.9998 - dense_2_acc_5: 0.9920 - dense_2_acc_6: 1.0000 - dense_2_acc_7: 1.0000 - dense_2_acc_8: 1.0000\n",
      "Epoch 11/1000\n",
      "1000/1000 [==============================] - 248s 248ms/step - loss: 0.0242 - dense_2_loss_1: 0.0019 - dense_2_loss_2: 0.0052 - dense_2_loss_3: 2.4672e-05 - dense_2_loss_4: 0.0019 - dense_2_loss_5: 0.0130 - dense_2_loss_6: 2.9027e-05 - dense_2_loss_7: 0.0013 - dense_2_loss_8: 8.5076e-04 - dense_2_acc_1: 0.9999 - dense_2_acc_2: 0.9997 - dense_2_acc_3: 1.0000 - dense_2_acc_4: 0.9999 - dense_2_acc_5: 0.9918 - dense_2_acc_6: 1.0000 - dense_2_acc_7: 0.9999 - dense_2_acc_8: 1.0000\n",
      "Epoch 12/1000\n",
      "1000/1000 [==============================] - 247s 247ms/step - loss: 0.0215 - dense_2_loss_1: 0.0016 - dense_2_loss_2: 0.0043 - dense_2_loss_3: 1.8571e-05 - dense_2_loss_4: 0.0014 - dense_2_loss_5: 0.0127 - dense_2_loss_6: 2.2478e-05 - dense_2_loss_7: 9.0322e-04 - dense_2_loss_8: 5.6588e-04 - dense_2_acc_1: 0.9999 - dense_2_acc_2: 0.9998 - dense_2_acc_3: 1.0000 - dense_2_acc_4: 0.9999 - dense_2_acc_5: 0.9917 - dense_2_acc_6: 1.0000 - dense_2_acc_7: 1.0000 - dense_2_acc_8: 1.0000\n",
      "Epoch 13/1000\n",
      "1000/1000 [==============================] - 248s 248ms/step - loss: 0.0196 - dense_2_loss_1: 0.0013 - dense_2_loss_2: 0.0035 - dense_2_loss_3: 1.4982e-05 - dense_2_loss_4: 0.0011 - dense_2_loss_5: 0.0125 - dense_2_loss_6: 1.7286e-05 - dense_2_loss_7: 7.1088e-04 - dense_2_loss_8: 4.6631e-04 - dense_2_acc_1: 1.0000 - dense_2_acc_2: 0.9998 - dense_2_acc_3: 1.0000 - dense_2_acc_4: 1.0000 - dense_2_acc_5: 0.9918 - dense_2_acc_6: 1.0000 - dense_2_acc_7: 1.0000 - dense_2_acc_8: 1.0000\n",
      "Epoch 14/1000\n",
      "1000/1000 [==============================] - 248s 248ms/step - loss: 0.0185 - dense_2_loss_1: 0.0011 - dense_2_loss_2: 0.0030 - dense_2_loss_3: 1.1139e-05 - dense_2_loss_4: 9.1673e-04 - dense_2_loss_5: 0.0124 - dense_2_loss_6: 1.3633e-05 - dense_2_loss_7: 6.3915e-04 - dense_2_loss_8: 4.5593e-04 - dense_2_acc_1: 1.0000 - dense_2_acc_2: 0.9998 - dense_2_acc_3: 1.0000 - dense_2_acc_4: 1.0000 - dense_2_acc_5: 0.9917 - dense_2_acc_6: 1.0000 - dense_2_acc_7: 1.0000 - dense_2_acc_8: 1.0000\n",
      "Epoch 15/1000\n",
      "1000/1000 [==============================] - 248s 248ms/step - loss: 0.0174 - dense_2_loss_1: 9.7354e-04 - dense_2_loss_2: 0.0026 - dense_2_loss_3: 8.7832e-06 - dense_2_loss_4: 7.6448e-04 - dense_2_loss_5: 0.0120 - dense_2_loss_6: 1.0413e-05 - dense_2_loss_7: 5.5757e-04 - dense_2_loss_8: 3.8211e-04 - dense_2_acc_1: 1.0000 - dense_2_acc_2: 0.9998 - dense_2_acc_3: 1.0000 - dense_2_acc_4: 1.0000 - dense_2_acc_5: 0.9919 - dense_2_acc_6: 1.0000 - dense_2_acc_7: 1.0000 - dense_2_acc_8: 1.0000\n",
      "Epoch 16/1000\n",
      "1000/1000 [==============================] - 248s 248ms/step - loss: 0.0167 - dense_2_loss_1: 8.3340e-04 - dense_2_loss_2: 0.0022 - dense_2_loss_3: 6.8328e-06 - dense_2_loss_4: 6.3816e-04 - dense_2_loss_5: 0.0122 - dense_2_loss_6: 8.6537e-06 - dense_2_loss_7: 4.7365e-04 - dense_2_loss_8: 3.2326e-04 - dense_2_acc_1: 1.0000 - dense_2_acc_2: 0.9998 - dense_2_acc_3: 1.0000 - dense_2_acc_4: 1.0000 - dense_2_acc_5: 0.9917 - dense_2_acc_6: 1.0000 - dense_2_acc_7: 1.0000 - dense_2_acc_8: 1.0000\n",
      "Epoch 17/1000\n",
      "1000/1000 [==============================] - 247s 247ms/step - loss: 0.0159 - dense_2_loss_1: 7.3208e-04 - dense_2_loss_2: 0.0019 - dense_2_loss_3: 5.4444e-06 - dense_2_loss_4: 5.5634e-04 - dense_2_loss_5: 0.0120 - dense_2_loss_6: 7.2246e-06 - dense_2_loss_7: 3.8762e-04 - dense_2_loss_8: 2.6383e-04 - dense_2_acc_1: 1.0000 - dense_2_acc_2: 0.9999 - dense_2_acc_3: 1.0000 - dense_2_acc_4: 1.0000 - dense_2_acc_5: 0.9918 - dense_2_acc_6: 1.0000 - dense_2_acc_7: 1.0000 - dense_2_acc_8: 1.0000\n",
      "Epoch 18/1000\n",
      "1000/1000 [==============================] - 247s 247ms/step - loss: 0.0157 - dense_2_loss_1: 6.6813e-04 - dense_2_loss_2: 0.0018 - dense_2_loss_3: 4.3802e-06 - dense_2_loss_4: 4.7774e-04 - dense_2_loss_5: 0.0121 - dense_2_loss_6: 5.6568e-06 - dense_2_loss_7: 3.4560e-04 - dense_2_loss_8: 2.3032e-04 - dense_2_acc_1: 1.0000 - dense_2_acc_2: 0.9999 - dense_2_acc_3: 1.0000 - dense_2_acc_4: 1.0000 - dense_2_acc_5: 0.9917 - dense_2_acc_6: 1.0000 - dense_2_acc_7: 1.0000 - dense_2_acc_8: 1.0000\n",
      "Epoch 19/1000\n",
      "1000/1000 [==============================] - 248s 248ms/step - loss: 0.0148 - dense_2_loss_1: 5.7071e-04 - dense_2_loss_2: 0.0015 - dense_2_loss_3: 3.4837e-06 - dense_2_loss_4: 4.2037e-04 - dense_2_loss_5: 0.0119 - dense_2_loss_6: 4.8151e-06 - dense_2_loss_7: 2.8456e-04 - dense_2_loss_8: 1.9338e-04 - dense_2_acc_1: 1.0000 - dense_2_acc_2: 0.9999 - dense_2_acc_3: 1.0000 - dense_2_acc_4: 1.0000 - dense_2_acc_5: 0.9918 - dense_2_acc_6: 1.0000 - dense_2_acc_7: 1.0000 - dense_2_acc_8: 1.0000\n",
      "Epoch 20/1000\n",
      "1000/1000 [==============================] - 247s 247ms/step - loss: 0.0147 - dense_2_loss_1: 5.3906e-04 - dense_2_loss_2: 0.0013 - dense_2_loss_3: 2.9130e-06 - dense_2_loss_4: 3.8323e-04 - dense_2_loss_5: 0.0120 - dense_2_loss_6: 3.9204e-06 - dense_2_loss_7: 2.9096e-04 - dense_2_loss_8: 1.9492e-04 - dense_2_acc_1: 1.0000 - dense_2_acc_2: 0.9999 - dense_2_acc_3: 1.0000 - dense_2_acc_4: 1.0000 - dense_2_acc_5: 0.9917 - dense_2_acc_6: 1.0000 - dense_2_acc_7: 1.0000 - dense_2_acc_8: 1.0000\n",
      "Epoch 21/1000\n",
      "1000/1000 [==============================] - 247s 247ms/step - loss: 0.0140 - dense_2_loss_1: 4.9894e-04 - dense_2_loss_2: 0.0013 - dense_2_loss_3: 2.4458e-06 - dense_2_loss_4: 3.4336e-04 - dense_2_loss_5: 0.0116 - dense_2_loss_6: 3.3819e-06 - dense_2_loss_7: 2.2842e-04 - dense_2_loss_8: 1.4699e-04 - dense_2_acc_1: 1.0000 - dense_2_acc_2: 0.9999 - dense_2_acc_3: 1.0000 - dense_2_acc_4: 1.0000 - dense_2_acc_5: 0.9919 - dense_2_acc_6: 1.0000 - dense_2_acc_7: 1.0000 - dense_2_acc_8: 1.0000\n",
      "Epoch 22/1000\n",
      "1000/1000 [==============================] - 247s 247ms/step - loss: 0.0139 - dense_2_loss_1: 4.7133e-04 - dense_2_loss_2: 0.0011 - dense_2_loss_3: 2.1364e-06 - dense_2_loss_4: 3.0829e-04 - dense_2_loss_5: 0.0117 - dense_2_loss_6: 2.9157e-06 - dense_2_loss_7: 2.1159e-04 - dense_2_loss_8: 1.4179e-04 - dense_2_acc_1: 0.9999 - dense_2_acc_2: 0.9999 - dense_2_acc_3: 1.0000 - dense_2_acc_4: 1.0000 - dense_2_acc_5: 0.9918 - dense_2_acc_6: 1.0000 - dense_2_acc_7: 1.0000 - dense_2_acc_8: 1.0000\n",
      "Epoch 23/1000\n",
      "1000/1000 [==============================] - 247s 247ms/step - loss: 0.0138 - dense_2_loss_1: 4.2775e-04 - dense_2_loss_2: 0.0010 - dense_2_loss_3: 1.7690e-06 - dense_2_loss_4: 2.8269e-04 - dense_2_loss_5: 0.0117 - dense_2_loss_6: 2.5176e-06 - dense_2_loss_7: 2.7531e-04 - dense_2_loss_8: 1.6137e-04 - dense_2_acc_1: 1.0000 - dense_2_acc_2: 0.9999 - dense_2_acc_3: 1.0000 - dense_2_acc_4: 1.0000 - dense_2_acc_5: 0.9919 - dense_2_acc_6: 1.0000 - dense_2_acc_7: 1.0000 - dense_2_acc_8: 1.0000\n",
      "Epoch 24/1000\n",
      "1000/1000 [==============================] - 247s 247ms/step - loss: 0.0137 - dense_2_loss_1: 3.9876e-04 - dense_2_loss_2: 9.5761e-04 - dense_2_loss_3: 1.5793e-06 - dense_2_loss_4: 2.6020e-04 - dense_2_loss_5: 0.0118 - dense_2_loss_6: 2.2529e-06 - dense_2_loss_7: 1.8080e-04 - dense_2_loss_8: 1.1964e-04 - dense_2_acc_1: 1.0000 - dense_2_acc_2: 0.9999 - dense_2_acc_3: 1.0000 - dense_2_acc_4: 1.0000 - dense_2_acc_5: 0.9916 - dense_2_acc_6: 1.0000 - dense_2_acc_7: 1.0000 - dense_2_acc_8: 1.0000\n",
      "Epoch 25/1000\n",
      "1000/1000 [==============================] - 247s 247ms/step - loss: 0.0135 - dense_2_loss_1: 3.5539e-04 - dense_2_loss_2: 8.6616e-04 - dense_2_loss_3: 1.3973e-06 - dense_2_loss_4: 2.3607e-04 - dense_2_loss_5: 0.0118 - dense_2_loss_6: 1.9051e-06 - dense_2_loss_7: 1.6885e-04 - dense_2_loss_8: 1.0862e-04 - dense_2_acc_1: 1.0000 - dense_2_acc_2: 0.9999 - dense_2_acc_3: 1.0000 - dense_2_acc_4: 1.0000 - dense_2_acc_5: 0.9917 - dense_2_acc_6: 1.0000 - dense_2_acc_7: 1.0000 - dense_2_acc_8: 1.0000\n",
      "Epoch 26/1000\n",
      "1000/1000 [==============================] - 247s 247ms/step - loss: 0.0145 - dense_2_loss_1: 6.9707e-04 - dense_2_loss_2: 0.0014 - dense_2_loss_3: 1.3755e-06 - dense_2_loss_4: 2.2458e-04 - dense_2_loss_5: 0.0117 - dense_2_loss_6: 1.8626e-06 - dense_2_loss_7: 2.9794e-04 - dense_2_loss_8: 1.7790e-04 - dense_2_acc_1: 0.9999 - dense_2_acc_2: 0.9998 - dense_2_acc_3: 1.0000 - dense_2_acc_4: 1.0000 - dense_2_acc_5: 0.9918 - dense_2_acc_6: 1.0000 - dense_2_acc_7: 1.0000 - dense_2_acc_8: 1.0000\n",
      "Epoch 27/1000\n",
      "1000/1000 [==============================] - 247s 247ms/step - loss: 0.0132 - dense_2_loss_1: 3.3005e-04 - dense_2_loss_2: 7.7833e-04 - dense_2_loss_3: 1.2072e-06 - dense_2_loss_4: 2.0808e-04 - dense_2_loss_5: 0.0117 - dense_2_loss_6: 1.5978e-06 - dense_2_loss_7: 1.3777e-04 - dense_2_loss_8: 9.4351e-05 - dense_2_acc_1: 1.0000 - dense_2_acc_2: 0.9999 - dense_2_acc_3: 1.0000 - dense_2_acc_4: 1.0000 - dense_2_acc_5: 0.9917 - dense_2_acc_6: 1.0000 - dense_2_acc_7: 1.0000 - dense_2_acc_8: 1.0000\n",
      "Epoch 28/1000\n",
      "1000/1000 [==============================] - 247s 247ms/step - loss: 0.0132 - dense_2_loss_1: 3.3946e-04 - dense_2_loss_2: 7.6326e-04 - dense_2_loss_3: 1.1670e-06 - dense_2_loss_4: 1.9854e-04 - dense_2_loss_5: 0.0117 - dense_2_loss_6: 4.3479e-06 - dense_2_loss_7: 1.3273e-04 - dense_2_loss_8: 9.0869e-05 - dense_2_acc_1: 1.0000 - dense_2_acc_2: 0.9999 - dense_2_acc_3: 1.0000 - dense_2_acc_4: 1.0000 - dense_2_acc_5: 0.9918 - dense_2_acc_6: 1.0000 - dense_2_acc_7: 1.0000 - dense_2_acc_8: 1.0000\n",
      "Epoch 29/1000\n",
      "1000/1000 [==============================] - 247s 247ms/step - loss: 0.0131 - dense_2_loss_1: 3.0931e-04 - dense_2_loss_2: 7.4364e-04 - dense_2_loss_3: 1.0797e-06 - dense_2_loss_4: 1.8945e-04 - dense_2_loss_5: 0.0117 - dense_2_loss_6: 1.4362e-06 - dense_2_loss_7: 1.3022e-04 - dense_2_loss_8: 8.7099e-05 - dense_2_acc_1: 1.0000 - dense_2_acc_2: 0.9999 - dense_2_acc_3: 1.0000 - dense_2_acc_4: 1.0000 - dense_2_acc_5: 0.9918 - dense_2_acc_6: 1.0000 - dense_2_acc_7: 1.0000 - dense_2_acc_8: 1.0000\n",
      "Epoch 30/1000\n",
      "1000/1000 [==============================] - 248s 248ms/step - loss: 0.0129 - dense_2_loss_1: 3.0560e-04 - dense_2_loss_2: 6.8080e-04 - dense_2_loss_3: 1.0900e-06 - dense_2_loss_4: 1.7490e-04 - dense_2_loss_5: 0.0115 - dense_2_loss_6: 1.3785e-06 - dense_2_loss_7: 1.2378e-04 - dense_2_loss_8: 8.3034e-05 - dense_2_acc_1: 1.0000 - dense_2_acc_2: 0.9999 - dense_2_acc_3: 1.0000 - dense_2_acc_4: 1.0000 - dense_2_acc_5: 0.9919 - dense_2_acc_6: 1.0000 - dense_2_acc_7: 1.0000 - dense_2_acc_8: 1.0000\n",
      "Epoch 31/1000\n",
      "1000/1000 [==============================] - 247s 247ms/step - loss: 0.0129 - dense_2_loss_1: 2.8407e-04 - dense_2_loss_2: 6.2887e-04 - dense_2_loss_3: 1.0149e-06 - dense_2_loss_4: 1.6463e-04 - dense_2_loss_5: 0.0116 - dense_2_loss_6: 1.3112e-06 - dense_2_loss_7: 1.2194e-04 - dense_2_loss_8: 8.1363e-05 - dense_2_acc_1: 1.0000 - dense_2_acc_2: 1.0000 - dense_2_acc_3: 1.0000 - dense_2_acc_4: 1.0000 - dense_2_acc_5: 0.9918 - dense_2_acc_6: 1.0000 - dense_2_acc_7: 1.0000 - dense_2_acc_8: 1.0000\n",
      "Epoch 32/1000\n",
      "1000/1000 [==============================] - 248s 248ms/step - loss: 0.0129 - dense_2_loss_1: 2.7955e-04 - dense_2_loss_2: 6.1717e-04 - dense_2_loss_3: 9.4517e-07 - dense_2_loss_4: 1.5832e-04 - dense_2_loss_5: 0.0115 - dense_2_loss_6: 1.2160e-06 - dense_2_loss_7: 1.5949e-04 - dense_2_loss_8: 1.0628e-04 - dense_2_acc_1: 1.0000 - dense_2_acc_2: 1.0000 - dense_2_acc_3: 1.0000 - dense_2_acc_4: 1.0000 - dense_2_acc_5: 0.9919 - dense_2_acc_6: 1.0000 - dense_2_acc_7: 1.0000 - dense_2_acc_8: 1.0000\n",
      "Epoch 33/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 247s 247ms/step - loss: 0.0127 - dense_2_loss_1: 2.6969e-04 - dense_2_loss_2: 5.9767e-04 - dense_2_loss_3: 8.6841e-07 - dense_2_loss_4: 1.5116e-04 - dense_2_loss_5: 0.0115 - dense_2_loss_6: 1.1560e-06 - dense_2_loss_7: 1.0697e-04 - dense_2_loss_8: 7.0767e-05 - dense_2_acc_1: 1.0000 - dense_2_acc_2: 1.0000 - dense_2_acc_3: 1.0000 - dense_2_acc_4: 1.0000 - dense_2_acc_5: 0.9920 - dense_2_acc_6: 1.0000 - dense_2_acc_7: 1.0000 - dense_2_acc_8: 1.0000\n",
      "Epoch 34/1000\n",
      "1000/1000 [==============================] - 247s 247ms/step - loss: 0.0131 - dense_2_loss_1: 3.3803e-04 - dense_2_loss_2: 8.1042e-04 - dense_2_loss_3: 7.2164e-07 - dense_2_loss_4: 1.4422e-04 - dense_2_loss_5: 0.0116 - dense_2_loss_6: 1.1019e-06 - dense_2_loss_7: 1.1393e-04 - dense_2_loss_8: 7.5664e-05 - dense_2_acc_1: 0.9999 - dense_2_acc_2: 0.9999 - dense_2_acc_3: 1.0000 - dense_2_acc_4: 1.0000 - dense_2_acc_5: 0.9918 - dense_2_acc_6: 1.0000 - dense_2_acc_7: 1.0000 - dense_2_acc_8: 1.0000\n",
      "Epoch 35/1000\n",
      "1000/1000 [==============================] - 247s 247ms/step - loss: 0.0127 - dense_2_loss_1: 2.4994e-04 - dense_2_loss_2: 5.6461e-04 - dense_2_loss_3: 7.0131e-07 - dense_2_loss_4: 1.3857e-04 - dense_2_loss_5: 0.0116 - dense_2_loss_6: 9.7825e-07 - dense_2_loss_7: 9.5737e-05 - dense_2_loss_8: 6.4275e-05 - dense_2_acc_1: 1.0000 - dense_2_acc_2: 1.0000 - dense_2_acc_3: 1.0000 - dense_2_acc_4: 1.0000 - dense_2_acc_5: 0.9919 - dense_2_acc_6: 1.0000 - dense_2_acc_7: 1.0000 - dense_2_acc_8: 1.0000\n",
      "Epoch 36/1000\n",
      "1000/1000 [==============================] - 247s 247ms/step - loss: 0.0126 - dense_2_loss_1: 2.4097e-04 - dense_2_loss_2: 5.3272e-04 - dense_2_loss_3: 6.9014e-07 - dense_2_loss_4: 1.3205e-04 - dense_2_loss_5: 0.0115 - dense_2_loss_6: 9.5316e-07 - dense_2_loss_7: 9.4824e-05 - dense_2_loss_8: 6.0040e-05 - dense_2_acc_1: 1.0000 - dense_2_acc_2: 1.0000 - dense_2_acc_3: 1.0000 - dense_2_acc_4: 1.0000 - dense_2_acc_5: 0.9918 - dense_2_acc_6: 1.0000 - dense_2_acc_7: 1.0000 - dense_2_acc_8: 1.0000\n",
      "Epoch 37/1000\n",
      "1000/1000 [==============================] - 247s 247ms/step - loss: 0.0125 - dense_2_loss_1: 2.3125e-04 - dense_2_loss_2: 5.0582e-04 - dense_2_loss_3: 6.5655e-07 - dense_2_loss_4: 1.2742e-04 - dense_2_loss_5: 0.0115 - dense_2_loss_6: 8.8117e-07 - dense_2_loss_7: 8.9269e-05 - dense_2_loss_8: 5.8636e-05 - dense_2_acc_1: 1.0000 - dense_2_acc_2: 1.0000 - dense_2_acc_3: 1.0000 - dense_2_acc_4: 1.0000 - dense_2_acc_5: 0.9918 - dense_2_acc_6: 1.0000 - dense_2_acc_7: 1.0000 - dense_2_acc_8: 1.0000\n",
      "Epoch 38/1000\n",
      "1000/1000 [==============================] - 247s 247ms/step - loss: 0.0126 - dense_2_loss_1: 2.3712e-04 - dense_2_loss_2: 5.0790e-04 - dense_2_loss_3: 5.9641e-07 - dense_2_loss_4: 1.3148e-04 - dense_2_loss_5: 0.0115 - dense_2_loss_6: 7.9226e-07 - dense_2_loss_7: 1.3520e-04 - dense_2_loss_8: 1.0329e-04 - dense_2_acc_1: 1.0000 - dense_2_acc_2: 1.0000 - dense_2_acc_3: 1.0000 - dense_2_acc_4: 1.0000 - dense_2_acc_5: 0.9919 - dense_2_acc_6: 1.0000 - dense_2_acc_7: 1.0000 - dense_2_acc_8: 1.0000\n",
      "Epoch 39/1000\n",
      "1000/1000 [==============================] - 247s 247ms/step - loss: 0.0124 - dense_2_loss_1: 2.3572e-04 - dense_2_loss_2: 5.0338e-04 - dense_2_loss_3: 6.3615e-07 - dense_2_loss_4: 1.2186e-04 - dense_2_loss_5: 0.0114 - dense_2_loss_6: 7.6320e-07 - dense_2_loss_7: 8.9078e-05 - dense_2_loss_8: 5.8443e-05 - dense_2_acc_1: 1.0000 - dense_2_acc_2: 1.0000 - dense_2_acc_3: 1.0000 - dense_2_acc_4: 1.0000 - dense_2_acc_5: 0.9919 - dense_2_acc_6: 1.0000 - dense_2_acc_7: 1.0000 - dense_2_acc_8: 1.0000\n",
      "Epoch 40/1000\n",
      "1000/1000 [==============================] - 247s 247ms/step - loss: 0.0126 - dense_2_loss_1: 2.0833e-04 - dense_2_loss_2: 4.7412e-04 - dense_2_loss_3: 6.1070e-07 - dense_2_loss_4: 1.1727e-04 - dense_2_loss_5: 0.0116 - dense_2_loss_6: 7.2064e-07 - dense_2_loss_7: 9.3868e-05 - dense_2_loss_8: 6.1776e-05 - dense_2_acc_1: 1.0000 - dense_2_acc_2: 1.0000 - dense_2_acc_3: 1.0000 - dense_2_acc_4: 1.0000 - dense_2_acc_5: 0.9917 - dense_2_acc_6: 1.0000 - dense_2_acc_7: 1.0000 - dense_2_acc_8: 1.0000\n",
      "Epoch 41/1000\n",
      "1000/1000 [==============================] - 247s 247ms/step - loss: 0.0125 - dense_2_loss_1: 2.0970e-04 - dense_2_loss_2: 4.5158e-04 - dense_2_loss_3: 5.2685e-07 - dense_2_loss_4: 1.1043e-04 - dense_2_loss_5: 0.0116 - dense_2_loss_6: 6.7128e-07 - dense_2_loss_7: 8.5127e-05 - dense_2_loss_8: 5.1756e-05 - dense_2_acc_1: 1.0000 - dense_2_acc_2: 1.0000 - dense_2_acc_3: 1.0000 - dense_2_acc_4: 1.0000 - dense_2_acc_5: 0.9917 - dense_2_acc_6: 1.0000 - dense_2_acc_7: 1.0000 - dense_2_acc_8: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc6816a8320>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "data_maker = dataset_generator(Tx, Ty, input_vocab, output_vocab, n_s, batch_size=1000)\n",
    "model.fit_generator(data_maker, steps_per_epoch=1000, epochs=1000, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "the time now is noon\n",
      "source: the time now is noon\n",
      "output: 11:58:00\n",
      "\n",
      "it is five o'clock\n",
      "source: it is five o'clock\n",
      "output: 13:09:55\n",
      "\n",
      "half past eleven o'clock in the morning\n",
      "source: half past eleven o'clock in the morning\n",
      "output: 11:30:00\n",
      "\n",
      "twenty nine minutes and fifty nine seconds past noon\n",
      "source: twenty nine minutes and fifty nine seconds past noon\n",
      "output: 12:29:59\n",
      "\n",
      "twenty nine minutes and fifty nine seconds past eleven o'clock at night\n",
      "source: twenty nine minutes and fifty nine seconds past eleven o'clock at night\n",
      "output: 23:29:59\n",
      "\n",
      "it is twenty eight minutes to noon\n",
      "source: it is twenty eight minutes to noon\n",
      "output: 13:59:22\n"
     ]
    }
   ],
   "source": [
    "EXAMPLES = [\n",
    "    'the time now is noon',\n",
    "    \"it is five o'clock\",\n",
    "    \"half past eleven o'clock in the morning\",\n",
    "    \"twenty nine minutes and fifty nine seconds past noon\",\n",
    "    \"twenty nine minutes and fifty nine seconds past eleven o'clock at night\",\n",
    "    \"it is twenty eight minutes to noon\"\n",
    "]\n",
    "\n",
    "np.set_printoptions(threshold=10)\n",
    "#print(human_vocab)\n",
    "#print('human', len(human_vocab))\n",
    "for example in EXAMPLES:\n",
    "    print()\n",
    "    print(example)\n",
    "    source = string_to_int(example, Tx, input_vocab)\n",
    "    \n",
    "    #print(source)\n",
    "    \n",
    "    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(input_vocab)), source))) #.swapaxes(0,1)\n",
    "             \n",
    "    #print(source.shape)\n",
    "    #print(source)\n",
    "    \n",
    "    prediction = model.predict([np.expand_dims(source, axis=0), np.zeros((1,n_s)), np.zeros((1,n_s))])\n",
    "    prediction = np.argmax(prediction, axis = -1)\n",
    "    output = [output_vocab_inv[int(i)] for i in prediction]\n",
    "    \n",
    "    print(\"source:\", example)\n",
    "    print(\"output:\", ''.join(output))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already exists themodel.h5\n",
      "already exists themodel-weights.h5\n"
     ]
    }
   ],
   "source": [
    "# happy?\n",
    "\n",
    "import os\n",
    "\n",
    "modelfile = \"themodel.h5\"\n",
    "if os.path.isfile(modelfile):\n",
    "    print('already exists', modelfile)\n",
    "else:\n",
    "    print('saving', modelfile)\n",
    "    model.save(modelfile)\n",
    "\n",
    "modelweightsfile = \"themodel-weights.h5\"\n",
    "if os.path.isfile(modelweightsfile):\n",
    "    print('already exists', modelweightsfile)\n",
    "else:\n",
    "    print('saving', modelweightsfile)\n",
    "    model.save_weights(modelweightsfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "See `attention.ipynb` for the attention map."
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "n16CQ",
   "launcher_item_id": "npjGi"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
